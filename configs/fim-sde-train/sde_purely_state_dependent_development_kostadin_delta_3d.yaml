seed: 0

model_description: SDE_purely_state_dependent

experiment_description: lr_1e5_linear_spatial_embed_4x8x256_transformer

model:
  ###################################################################################
  # To change
  #
  # phi_0^t
  temporal_embedding_size:
    &temporal_embedding_size 256

    # phi_0^s
  spatial_embedding_size: &spatial_embedding_size 256
  spatial_embedding_hidden_layers: &spatial_embedding_hidden_layers null #  if null, this will just be dense layer

  # psi_1
  sequence_encoding_transformer_hidden_size: &sequence_encoding_transformer_hidden_size 256
  sequence_encoding_transformer_heads: &sequence_encoding_transformer_heads 8
  sequence_encoding_transformer_layers: &sequence_encoding_transformer_layers 4

  # Omega_1
  combining_transformer_hidden_size: &combining_transformer_hidden_size 256
  combining_transformer_heads: &combining_transformer_heads 8
  combining_transformer_layers: &combining_transformer_layers 4

  # phi_1
  trunk_net_size: &trunk_net_size 576 #needs to be divisible by 12, as this is the dimensionality of the dot-products
  trunk_net_hidden_layers: &trunk_net_hidden_layers [256, 256]

  # optimizer + regularising
  add_delta_x_to_value_encoder: &add_delta_x_to_value_encoder true
  learning_rate: &learning_rate 1.0e-5
  weight_decay: &weight_decay 1.0e-4
  dropout_rate: &dropout_rate 0.1

  diffusion_loss_scale: &diffusion_loss_scale 1.0
  loss_threshold: &loss_threshold 100.0

  # data loading
  dataset_description: &dataset_description SDE_linear_SNR_01_05_1_5_DELTA_3D
  total_minibatch_size: &total_minibatch_size 32
  total_minibatch_size_test: &total_minibatch_size_test 64
  max_number_of_paths: &max_number_of_paths 300

  tensorboard_figure_data: &tensorboard_figure_data "test" # "test" or "train"
  plot_paths_count: &plot_paths_count 100

  data_loading_processes_count: &data_loading_processes_count 0

  data_in_files: &data_in_files
    obs_times: "obs_times.h5"
    obs_values: "obs_values.h5"
    locations: "hypercube_locations.h5"
    drift_at_locations: "drift_functions_at_hypercube.h5"
    diffusion_at_locations: "scaled_diffusion_functions_at_hypercube.h5"

  dataset_path_collections: &dataset_path_collections
    train:
      - /cephfs_projects/foundation_models/data/SDE_concepts/08-09-24-with_delta_x/data-snr_01_05_1_5/linear/dim-3/1

    test:
      - /cephfs_projects/foundation_models/data/SDE_concepts/08-09-24-with_delta_x/data-snr_01_05_1_5/linear/dim-3/2

  ###################################################################################

  class_path: wienerfm.models.sde_inference.SDEInference
  detect_nan_outputs: true
  return_nan_loss_stats: true

  # training
  loss_on_normalized_data: true
  active_concept_train_loss: rmse
  global_loss_threshold: *loss_threshold

  # normalization methods
  times_normalization_method: min_max
  values_normalization_method: min_max

  # concepts to infer
  concept_type: purely_state_dependent
  max_process_dim: &max_process_dim 3

  # precision in forward pass and of parameters
  dtype: &dtype bfloat16 # bfloat16
  param_dtype: &param_dtype float32

  layer_norm_config: &layer_norm_config
    dtype: *dtype
    param_dtype: *param_dtype

  dense_config: &dense_config
    dtype: *dtype
    param_dtype: *param_dtype

  mlp_layer_config: &mlp_layer_config
    activation: &activation silu
    dropout: &dropout
      rate: *dropout_rate
    dense: *dense_config

  final_layer: &mlp_final_layer_config
    dense: *dense_config

  neural_operator:
    class_path: wienerfm.blocks.encoders.DeepONetWithTimeDomainSeparation

    add_delta_x_to_value_encoder: *add_delta_x_to_value_encoder

    time_encoder:
      class_path: wienerfm.blocks.encoders.LearnedSinusTimeEncoder
      encoding_dim: *temporal_embedding_size
      dtype: *dtype
      param_dtype: *param_dtype

    value_encoder:
      class_path: wienerfm.blocks.encoders.MLPPositionalEncoderAdapter
      mlp_like:
        class_path: wienerfm.blocks.mlp.MLP
        hidden_layers: *spatial_embedding_hidden_layers
        output_size: *spatial_embedding_size
        layers: *mlp_layer_config
        final_layer: *mlp_final_layer_config

    locations_encoder: # trunk net equivalent, otherwise defaults to value encoder
      class_path: wienerfm.blocks.encoders.MLPPositionalEncoderAdapter
      mlp_like:
        class_path: wienerfm.blocks.mlp.MLP
        hidden_layers: *trunk_net_hidden_layers
        output_size: *trunk_net_size # needs to be hidden_size of neural operator
        layers: *mlp_layer_config
        final_layer: *mlp_final_layer_config

    encoder:
      class_path: wienerfm.blocks.attention.AttentionEncoderOldInterface
      output_size: *sequence_encoding_transformer_hidden_size
      layers_count: *sequence_encoding_transformer_layers
      dropout_rate: *dropout_rate
      dense: *dense_config
      attention:
        num_heads: *sequence_encoding_transformer_heads
        dtype: *dtype
        param_dtype: *param_dtype
        force_fp32_for_softmax: true
      residual_feed_forward: &residual_feed_forward
        activation: *activation

    sequence_encoder: # processes all paths separately
      class_path: wienerfm.blocks.encoders.MultiPathEncoder

      embedding_combiner: # combines embeddings of multiple paths to a single embedding
        class_path: wienerfm.blocks.attention.AttentionEncoder
        output_size: *combining_transformer_hidden_size
        layers_count: *combining_transformer_layers
        dropout_rate: *dropout_rate
        dense: *dense_config
        attention:
          num_heads: *combining_transformer_heads
          dtype: *dtype
          param_dtype: *param_dtype
          force_fp32_for_softmax: true
        residual_feed_forward: *residual_feed_forward

    branch_projection:
      class_path: wienerfm.blocks.mlp.MLP
      output_size: *trunk_net_size
      hidden_layers: null
      dense:
        dtype: *dtype
        param_dtype: *param_dtype
