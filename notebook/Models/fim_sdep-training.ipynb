{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from pathlib import Path\n",
    "import pytorch_lightning as pl\n",
    "from dataclasses import dataclass\n",
    "from dataclasses import dataclass,asdict, field\n",
    "from typing import Any, Dict, Optional, Union, List,Tuple\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from fimodemix.configs.config_classes.fim_sde_config import FIMSDEModelParams\n",
    "from fimodemix.data.dataloaders import (\n",
    "    FIMSDEpDataLoader\n",
    ")\n",
    "from fimodemix.utils.experiment_files import ExperimentsFiles\n",
    "from fimodemix.models.fim_sde import FIMSDEp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment\n",
    "experiment_files = ExperimentsFiles(experiment_indentifier=None,\n",
    "                                    delete=True)\n",
    "# Set up TensorBoard logger\n",
    "logger = MLFlowLogger(experiment_name=\"time_series_transformer\",\n",
    "                      tracking_uri=\"http://localhost:5000\",\n",
    "                      save_dir=\"mlruns\")\n",
    "\n",
    "# Set up Model Checkpointing\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='train_loss',\n",
    "    dirpath=experiment_files.checkpoints_dir,\n",
    "    filename='best-checkpoint-hey',\n",
    "    save_top_k=1,\n",
    "    mode='min',\n",
    "    save_weights_only=True,\n",
    "    every_n_train_steps=100  # Save checkpoint every 100 training steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cesar\\desktop\\projects\\foundationmodels\\fimodemix\\src\\fimodemix\\data\\datasets.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data: FIMSDEpDatabatch = torch.load(file_path)  # Adjust loading method as necessary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Hypercube Size: 1024\n",
      "Max Dimension: 3\n",
      "Max Num Steps: 129\n",
      "Max Hypercube Size: 1024\n",
      "Max Dimension: 3\n",
      "Max Num Steps: 129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cesar\\anaconda3\\envs\\fimode\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Hypercube Size: 1024\n",
      "Max Dimension: 3\n",
      "Max Num Steps: 129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment with name time_series_transformer not found. Creating it.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | phi_t0    | TimeEncoding       | 38     | train\n",
      "1 | phi_x0    | Mlp                | 3.8 K  | train\n",
      "2 | phi_1     | Mlp                | 2.9 K  | train\n",
      "3 | phi_2     | Mlp                | 4.8 K  | train\n",
      "4 | psi1      | TransformerModel   | 93.2 K | train\n",
      "5 | query_1x  | QueryGenerator     | 160    | train\n",
      "6 | query_1   | StaticQuery        | 5.2 K  | train\n",
      "7 | omega_1   | MultiheadAttention | 6.6 K  | train\n",
      "8 | criterion | MSELoss            | 0      | train\n",
      "---------------------------------------------------------\n",
      "116 K     Trainable params\n",
      "0         Non-trainable params\n",
      "116 K     Total params\n",
      "0.466     Total estimated model params size (MB)\n",
      "61        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\cesar\\anaconda3\\envs\\fimode\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   4%|â–Ž         | 11/313 [00:01<00:27, 10.81it/s, v_num=def6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cesar\\anaconda3\\envs\\fimode\\Lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:04<00:00, 74.97it/s, v_num=def6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:04<00:00, 74.94it/s, v_num=def6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/15 14:52:15 INFO mlflow.tracking._tracking_service.client: ðŸƒ View run youthful-fox-866 at: http://localhost:5000/#/experiments/791522484725792022/runs/76769b9823364aa7bab16466881edef6.\n",
      "2024/10/15 14:52:15 INFO mlflow.tracking._tracking_service.client: ðŸ§ª View experiment at: http://localhost:5000/#/experiments/791522484725792022.\n"
     ]
    }
   ],
   "source": [
    "params = FIMSDEModelParams()\n",
    "\n",
    "#Set up Dataloaders\n",
    "dataloaders = FIMSDEpDataLoader(params)\n",
    "\n",
    "# Set up Model\n",
    "model = FIMSDEp(params)\n",
    "\n",
    "#Set up trainers\n",
    "trainer = Trainer(\n",
    "    max_epochs=params.num_epochs,\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "trainer.fit(model, dataloaders.train_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.log_params(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fimode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
