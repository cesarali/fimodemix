{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from pprint import pprint\n",
    "from dataclasses import dataclass,asdict, field\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from typing import List\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from fimodemix.data.datasets import (\n",
    "    FIMSDEDataset,\n",
    "    FIMSDEDatabatch\n",
    ")\n",
    "\n",
    "from fimodemix.utils.experiment_files import ExperimentsFiles\n",
    "from fimodemix.models.blocks import (\n",
    "    TimeEncoding,\n",
    "    Mlp,\n",
    "    TransformerModel\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class ModelParams:\n",
    "    # data\n",
    "    input_size: int = 1  # Original input size\n",
    "\n",
    "    max_dimension:int = 1\n",
    "    max_hypercube_size:int = 1\n",
    "    max_num_steps:int = 1\n",
    "\n",
    "    # model architecture\n",
    "    dim_time:int = 19\n",
    "\n",
    "    # phi_0 / first data encoding\n",
    "    x0_hidden_layers: List[int] = field(default_factory=lambda:[50,50])\n",
    "    x0_out_features: int = 21\n",
    "    x0_dropout: float = 0.2\n",
    "\n",
    "    encoding0_dim:int = 40 #  x0_out_features + dim_time\n",
    "\n",
    "    #psi_1 / first transformer\n",
    "    psi1_nhead:int = 2\n",
    "    psi1_hidden_dim:int = 300\n",
    "    psi1_nlayers:int = 2\n",
    "\n",
    "    #Multiheaded Attention 1 / first path summary\n",
    "    query_dim:int = 10\n",
    "\n",
    "    n_heads: int = 4\n",
    "    hidden_dim: int = 64\n",
    "    output_size: int = 1\n",
    "    batch_size: int = 32\n",
    "    seq_length: int = 10\n",
    "\n",
    "    # training\n",
    "    num_epochs: int = 10\n",
    "    learning_rate: float = 0.001\n",
    "    embed_dim: int = 8  # New embedding dimension\n",
    "\n",
    "    def __post__init__(self):\n",
    "        self.encoding0_dim = self.x0_out_features + self.dim_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformer(pl.LightningModule):\n",
    "    def __init__(self, input_size, n_heads, hidden_dim, output_size, embed_dim):\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Linear(input_size, embed_dim)  # Transform input to embed_dim\n",
    "        assert embed_dim % n_heads == 0, \"embed_dim must be divisible by n_heads\"\n",
    "        \n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=n_heads)\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Transform to (batch_size, seq_length, embed_dim)\n",
    "        x = x.permute(1, 0, 2)  # Change to (seq_length, batch_size, embed_dim)\n",
    "        \n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = attn_output.permute(1, 0, 2)  # Back to (batch_size, seq_length, embed_dim)\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self.forward(inputs)\n",
    "        loss = self.criterion(outputs, targets)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define your query generation model (a simple linear layer can work)\n",
    "class QueryGenerator(nn.Module):\n",
    "    def __init__(self, input_dim, query_dim):\n",
    "        super(QueryGenerator, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, query_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 1. Define a static query matrix as a learnable parameter\n",
    "class StaticQuery(nn.Module):\n",
    "    def __init__(self, num_steps, query_dim):\n",
    "        super(StaticQuery, self).__init__()\n",
    "        self.queries = nn.Parameter(torch.randn(num_steps, query_dim))  # Learnable queries\n",
    "\n",
    "    def forward(self):\n",
    "        return self.queries\n",
    "    \n",
    "# 5. Apply multi-headed attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "\n",
    "    def forward(self, query, key_value):\n",
    "        # Use the same keys and values\n",
    "        attn_output, _ = self.attention(query, key_value, key_value)\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Hypercube Size: 1024\n",
      "Max Dimension: 3\n",
      "Max Num Steps: 129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cesar\\desktop\\projects\\foundationmodels\\fimodemix\\src\\fimodemix\\data\\datasets.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data: FIMSDEDatabatch = torch.load(file_path)  # Adjust loading method as necessary\n"
     ]
    }
   ],
   "source": [
    "params = ModelParams()\n",
    "# Example usage:\n",
    "dataset = FIMSDEDataset(params=params)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=24, shuffle=True)\n",
    "databatch = next(data_loader.__iter__())\n",
    "obs_values, obs_times, diffusion_at_hypercube, drift_at_hypercube, hypercube_locations, mask = databatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = obs_times.size(0)\n",
    "num_steps = obs_times.size(1)\n",
    "dimensions = obs_values.size(2)\n",
    "num_hyper = hypercube_locations.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cesar\\anaconda3\\envs\\fimode\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "phi_t0 = TimeEncoding(params.dim_time)\n",
    "\n",
    "phi_x0 = Mlp(in_features=params.max_dimension,\n",
    "             out_features=params.x0_out_features,\n",
    "             hidden_layers=params.x0_hidden_layers,\n",
    "             output_act=nn.SiLU())\n",
    "\n",
    "phi_1 = Mlp(in_features=params.max_dimension,\n",
    "            out_features=params.max_dimension,\n",
    "            hidden_layers=params.x0_hidden_layers)\n",
    "\n",
    "phi_2 = Mlp(in_features=params.encoding0_dim,\n",
    "            out_features=params.max_dimension,\n",
    "            hidden_layers=params.x0_hidden_layers)\n",
    "\n",
    "psi1 = TransformerModel(input_dim=params.encoding0_dim, \n",
    "                        nhead=params.psi1_nhead, \n",
    "                        hidden_dim=params.psi1_hidden_dim, \n",
    "                        nlayers=params.psi1_nlayers)\n",
    "\n",
    "\n",
    "queries = nn.Parameter(torch.randn(1, params.encoding0_dim))\n",
    "\n",
    "query_1x = QueryGenerator(input_dim=params.max_dimension,\n",
    "                          query_dim=params.encoding0_dim)\n",
    "\n",
    "query_1 =  StaticQuery(num_steps=params.max_num_steps,\n",
    "                       query_dim=params.encoding0_dim)\n",
    "\n",
    "# Create the MultiheadAttention module\n",
    "omega_1 = nn.MultiheadAttention(params.encoding0_dim, params.psi1_nhead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\left| G'(u)(y) - \\sum_{k=1}^{p} \\sum_{i=1}^{n} c_{i}^{k} \\sigma \\left( \\sum_{j=1}^{m} \\epsilon_{ij}^{k} u(x_{j}) + \\theta_{i}^{k} \\right) \\sigma(u_{k} \\cdot y + \\zeta_{k}) \\right| < \\epsilon\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G -----------------------------------\n",
    "\n",
    "time_encoding_ = phi_t0(obs_times.reshape(batch_size*num_steps,-1)) #(batch_size*num_steps,dim_time)\n",
    "x_enconding = phi_x0(obs_values.reshape(batch_size*num_steps,-1)) #(batch_size*num_steps,x0_out_features)\n",
    "H = torch.cat([time_encoding_,x_enconding],dim=1) #(batch_size*num_steps,encoding0_dim)\n",
    "H  = H.reshape(batch_size,num_steps,params.encoding0_dim) \n",
    "H = psi1(torch.transpose(H,0,1)) # (seq_lenght,batch_size,encoding0_dim)\n",
    "\n",
    "# Reshape obs_values as needed to create queries\n",
    "obs_values_reshaped = obs_values.reshape(batch_size * num_steps, -1)\n",
    "\n",
    "# Trunk Queries ------------------\n",
    "hypercube_locations = hypercube_locations.reshape(batch_size*num_hyper,dimensions)\n",
    "tx = query_1x(hypercube_locations)  # Shape: (batch_size*num_steps, encoding0_dim)\n",
    "\n",
    "# Reshape queries to match the attention requirements\n",
    "tx = tx.reshape(num_hyper, batch_size, params.encoding0_dim)  # Shape: (num_hyper, batch_size, encoding0_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representation per path\n",
    "# attn_output, _ = multihead_attn(queries[:,None,:].repeat(1,batch_size,1), H, H) # Shape: (1, batch_size, query_dim)\n",
    "attn_output, _ = omega_1(tx, H, H) # Shape: (num_hyper, batch_size, query_dim)\n",
    "attn_output = torch.transpose(attn_output,1,0) # Shape: (num_hyper, batch_size, query_dim)\n",
    "attn_output = attn_output.reshape(num_hyper*batch_size,params.encoding0_dim)\n",
    "\n",
    "f_hat = phi_2(attn_output).reshape(batch_size,num_hyper,dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 1024, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drift_at_hypercube.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 1024, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\textbf{Architecture}:\n",
    "\\begin{enumerate}[(i)]    \\textit{Spatial embedding} $\\phi^s_0$. Currently $\\phi^s_0$ is a MLP with silu activation. \n",
    "    \\item \n",
    "    \n",
    "    \\item \\textit{Temporal embedding} $\\phi^t_0$. We use the time embedding of~\\citet{shukla2020multitime}.\n",
    "\n",
    "    \\item \\textit{Trunk net equivalent}. It's given by an MLP $\\phi_1$ which takes as input the embedded evaluation point. We denote it with\n",
    "    $$\n",
    "    \\mathbf{t}(\\mathbf{x}) = \\phi_1(\\phi^s_0(\\mathbf{x}))\n",
    "    $$    \n",
    "    \n",
    "    \\item \\textit{Embedded input}. Let us denote the $i$th element of the $k$th time series in our input time series with \n",
    "    %\n",
    "    $$\n",
    "        \\mathbf{u}_{ki} = \\text{Concat}(\\phi^s_0(\\mathbf{x}_{ki}, \\Delta \\mathbf{x}_{ki}, \\Delta \\mathbf{x}_{ki}^2, \\theta), \\phi^t_0(\\tau_{ki}, \\theta)).\n",
    "    $$\n",
    "    \n",
    "    \\item \\textit{Sequence processing network} $\\psi_1$. We process each path with a Transformer network $\\psi_1$ as follows\n",
    "    %\n",
    "    $$\\mathbf{h}_{k1}, \\dots, \\mathbf{h}_{kl} = \\psi_1(\\mathbf{u}_{k1}, \\dots, \\mathbf{u}_{kl}, \\theta), \\, \\, \\text{with} \\, \\, \\mathbf{h}_{ki} \\in \\mathbb{R}^{d_{att}}$$ \n",
    "\n",
    "    \\item \\textit{Path embedding}. Let's denote the output sequence of vectors ($\\mathbf{h}_{k1}, \\dots, \\mathbf{h}_{kl}$) for the $k$th path with the matrix of $H_k \\in \\mathbb{R}^{l \\times d_{att}}$. We summarize each path with an attention network\n",
    "\n",
    "    \\begin{equation}    \n",
    "    \\mathbf{h}_k(\\mathbf{x}) = \\Omega_1(\\mathbf{t}(\\mathbf{x}), H_k, H_k),\n",
    "    \\label{eq:simple-attention-q}\n",
    "    \\end{equation}    \n",
    "    where $\\mathbf{t}(\\mathbf{x}) \\in \\mathbb{R}^{1 \\times d_{att}}$ is the output of Trunk net. \n",
    "    %\n",
    "    In this way, we have a path embedding which depends on the location at which we want to evaluate the function.    \n",
    "    Note that, as usual, the $i$th attention network is given by\n",
    "    $$\n",
    "    \\Omega_i(Q, K, V) = \\text{softmax} \\left( d_{att}^{-1/2} Q \\cdot K^T\\right) \\cdot V. \n",
    "    $$    \n",
    "\n",
    "    \\item \\textit{Summary over paths}. We summarize the set of $K$ \\textit{state-dependent} path embeddings with\n",
    "    %\n",
    "    $$\n",
    "    \\mathbf{b}(\\mathbf{x}) = \\Omega_2(\\mathbf{q}, H(\\mathbf{x}), H(\\mathbf{x})),\n",
    "    $$\n",
    "    with $\\mathbf{q} \\in \\mathbb{R}^{1 \\times d_{att}}$ a learnable query, and $H(\\mathbf{x}) = \\mathbf{h}_1(\\mathbf{x}), \\dots, \\mathbf{h}_K(\\mathbf{x})$.\n",
    "    \n",
    "    \\item \\textit{Final layer}: We use a simple MLP $\\phi_2$ to project the final embedding: \n",
    "    $$\n",
    "    \\mathbf{\\hat f}(\\mathbf{x}), \\log \\text{Var}(\\mathbf{\\hat f})(\\mathbf{x}),  \\mathbf{\\hat g}(\\mathbf{x}), \\log \\text{Var}(\\mathbf{\\hat g})(\\mathbf{x}) = \\phi_2(\\mathbf{b}(\\mathbf{x})).\n",
    "    $$    \n",
    "\\end{enumerate}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fimode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
