{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from pathlib import Path\n",
    "import pytorch_lightning as pl\n",
    "from dataclasses import dataclass\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from fimodemix.data.datasets import (\n",
    "    FIMSDEDataset,\n",
    "    FIMSDEDatabatch\n",
    ")\n",
    "\n",
    "from fimodemix.utils.experiment_files import ExperimentsFiles\n",
    "from fimodemix.models.blocks import (\n",
    "    TimeEncoding,\n",
    "    Mlp,\n",
    "    TransformerModel\n",
    ")\n",
    "\n",
    "from typing import Any, Dict, Optional, Union, List,Tuple\n",
    "from dataclasses import dataclass,asdict, field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fimodemix import results_path\n",
    "\n",
    "class ExperimentsFiles:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,experiment_dir=None,experiment_indentifier=None,delete=False):\n",
    "        self.delete = delete\n",
    "        self.define_experiment_folder(experiment_dir,experiment_indentifier)\n",
    "        self.create_directories()\n",
    "\n",
    "    def define_experiment_folder(self,experiment_dir=None,experiment_indentifier=None):\n",
    "        if experiment_dir is None:\n",
    "            results_dir = str(results_path)\n",
    "            if experiment_indentifier is None:\n",
    "                experiment_indentifier = str(int(time.time()))\n",
    "            self.experiment_dir = os.path.join(results_dir, experiment_indentifier)        \n",
    "        self.tensorboard_dir = os.path.join(self.experiment_dir, \"logs\")\n",
    "        self.checkpoints_dir = os.path.join(self.experiment_dir, \"checkpoints\")\n",
    "    \n",
    "    def create_directories(self):\n",
    "        if not Path(self.experiment_dir).exists():\n",
    "            os.makedirs(self.experiment_dir)\n",
    "        else:\n",
    "            if self.delete:\n",
    "                shutil.rmtree(self.experiment_dir)\n",
    "                os.makedirs(self.experiment_dir)\n",
    "            else:\n",
    "                raise Exception(\"Folder Exist no Experiments Created Set Delete to True\")\n",
    "            \n",
    "        if not os.path.isdir(self.tensorboard_dir):\n",
    "            os.makedirs(self.tensorboard_dir)\n",
    "        \n",
    "        if not os.path.isdir(self.checkpoints_dir):\n",
    "            os.makedirs(self.checkpoints_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference/Sample Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FIMSDEModelParams:\n",
    "    # data\n",
    "    input_size: int = 1  # Original input size\n",
    "\n",
    "    max_dimension:int = 1\n",
    "    max_hypercube_size:int = 1\n",
    "    max_num_steps:int = 1\n",
    "\n",
    "    # model architecture\n",
    "    dim_time:int = 19\n",
    "\n",
    "    # phi_0 / first data encoding\n",
    "    x0_hidden_layers: List[int] = field(default_factory=lambda:[50,50])\n",
    "    x0_out_features: int = 21\n",
    "    x0_dropout: float = 0.2\n",
    "\n",
    "    encoding0_dim:int = 40 #  x0_out_features + dim_time\n",
    "\n",
    "    #psi_1 / first transformer\n",
    "    psi1_nhead:int = 2\n",
    "    psi1_hidden_dim:int = 300\n",
    "    psi1_nlayers:int = 2\n",
    "\n",
    "    #Multiheaded Attention 1 / first path summary\n",
    "    query_dim:int = 10\n",
    "\n",
    "    n_heads: int = 4\n",
    "    hidden_dim: int = 64\n",
    "    output_size: int = 1\n",
    "    batch_size: int = 32\n",
    "    seq_length: int = 10\n",
    "\n",
    "    # training\n",
    "    num_epochs: int = 10\n",
    "    learning_rate: float = 0.001\n",
    "    embed_dim: int = 8  # New embedding dimension\n",
    "\n",
    "    def __post__init__(self):\n",
    "        self.encoding0_dim = self.x0_out_features + self.dim_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = FIMSDEModelParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define your query generation model (a simple linear layer can work)\n",
    "class QueryGenerator(nn.Module):\n",
    "    def __init__(self, input_dim, query_dim):\n",
    "        super(QueryGenerator, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, query_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 2. Define a static query matrix as a learnable parameter\n",
    "class StaticQuery(nn.Module):\n",
    "    def __init__(self, num_steps, query_dim):\n",
    "        super(StaticQuery, self).__init__()\n",
    "        self.queries = nn.Parameter(torch.randn(num_steps, query_dim))  # Learnable queries\n",
    "\n",
    "    def forward(self):\n",
    "        return self.queries\n",
    "\n",
    "# 3. Model Following FIM conventions\n",
    "class FIMSDE_p(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    This is the more simple architecture for \n",
    "\n",
    "    Stochastic Differential Equation Trainining\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            params: str|FIMSDEModelParams,\n",
    "            device:torch.device = None\n",
    "        ):\n",
    "        super(FIMSDE_p, self).__init__()\n",
    "        self._create_model(params)\n",
    "        if device is not None:\n",
    "            self.to(device)\n",
    "\n",
    "    def _create_model(\n",
    "        self,\n",
    "        params: dict | FIMSDEModelParams,\n",
    "    ):\n",
    "        # Architecture ---------\n",
    "        self.phi_t0 = TimeEncoding(params.dim_time)\n",
    "\n",
    "        self.phi_x0 = Mlp(in_features=params.max_dimension,\n",
    "                          out_features=params.x0_out_features,\n",
    "                          hidden_layers=params.x0_hidden_layers,\n",
    "                          output_act=nn.SiLU())\n",
    "\n",
    "        self.phi_1 = Mlp(in_features=params.max_dimension,\n",
    "                         out_features=params.max_dimension,\n",
    "                         hidden_layers=params.x0_hidden_layers)\n",
    "\n",
    "        self.phi_2 = Mlp(in_features=params.encoding0_dim,\n",
    "                         out_features=params.max_dimension,\n",
    "                         hidden_layers=params.x0_hidden_layers)\n",
    "\n",
    "        self.psi1 = TransformerModel(input_dim=params.encoding0_dim, \n",
    "                                     nhead=params.psi1_nhead, \n",
    "                                     hidden_dim=params.psi1_hidden_dim, \n",
    "                                     nlayers=params.psi1_nlayers)\n",
    "        \n",
    "        #self.queries = nn.Parameter(torch.randn(1, params.encoding0_dim))\n",
    "        self.query_1x = QueryGenerator(input_dim=params.max_dimension,\n",
    "                                       query_dim=params.encoding0_dim)\n",
    "\n",
    "        self.query_1 =  StaticQuery(num_steps=params.max_num_steps,\n",
    "                            query_dim=params.encoding0_dim)\n",
    "\n",
    "        # Create the MultiheadAttention module\n",
    "        self.omega_1 = nn.MultiheadAttention(params.encoding0_dim, params.psi1_nhead)\n",
    "\n",
    "        # Loss ------------------\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            hypercube_locations:torch.tensor, \n",
    "            obs_values:torch.tensor, \n",
    "            obs_times:torch.tensor,\n",
    "            observation_mask:torch.tensor,\n",
    "            training:bool=True,\n",
    "            ) -> Tuple[torch.tensor,torch.tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            \n",
    "            hypercube_locations [B, H, D] observation values. optionally with noise.\n",
    "            obs_values [B, T, D] observation times\n",
    "            obs_times [B, T, D] \n",
    "            observation_mask, dtype: bool (0: value is observed, 1: value is masked out)\n",
    "            training (bool): flag indicating if model is in training mode. Has an impact on the output.\n",
    "            \n",
    "            with B: batch size, T: number of observation times, D: dimensionsm, H: number of fine grid points (locations)\n",
    "\n",
    "        Returns:\n",
    "            if training:\n",
    "                dict: losses\n",
    "            else:\n",
    "                dict: losses (if target drift is provided), metrics, visualizations data\n",
    "        \"\"\"\n",
    "        batch_size = obs_times.size(0)\n",
    "        num_steps = obs_times.size(1)\n",
    "        dimensions = obs_values.size(2)\n",
    "        num_hyper = hypercube_locations.size(1)\n",
    "\n",
    "        # Encoding Paths -----------------\n",
    "        time_encoding_ = self.phi_t0(obs_times.reshape(batch_size*num_steps,-1)) #(batch_size*num_steps,dim_time)\n",
    "        x_enconding = self.phi_x0(obs_values.reshape(batch_size*num_steps,-1)) #(batch_size*num_steps,x0_out_features)\n",
    "        H = torch.cat([time_encoding_,x_enconding],dim=1) #(batch_size*num_steps,encoding0_dim)\n",
    "        H  = H.reshape(batch_size,num_steps,params.encoding0_dim) \n",
    "        H = self.psi1(torch.transpose(H,0,1)) # (seq_lenght,batch_size,encoding0_dim)\n",
    "\n",
    "        # Trunk Queries ------------------\n",
    "        hypercube_locations = hypercube_locations.reshape(batch_size*num_hyper,dimensions)\n",
    "        tx = self.query_1x(hypercube_locations)  # Shape: (batch_size*num_steps, encoding0_dim)\n",
    "        # Reshape queries to match the attention requirements\n",
    "        tx = tx.reshape(num_hyper, batch_size, params.encoding0_dim)  # Shape: (num_hyper, batch_size, encoding0_dim)\n",
    "\n",
    "        # Representation per path\n",
    "        # attn_output, _ = multihead_attn(queries[:,None,:].repeat(1,batch_size,1), H, H) # Shape: (1, batch_size, query_dim)\n",
    "        attn_output, _ = self.omega_1(tx, H, H) # Shape: (num_hyper, batch_size, query_dim)\n",
    "        attn_output = torch.transpose(attn_output,1,0) # Shape: (num_hyper, batch_size, query_dim)\n",
    "        attn_output = attn_output.reshape(num_hyper*batch_size,params.encoding0_dim)\n",
    "\n",
    "        # obtain all heads\n",
    "        f_hat = self.phi_2(attn_output).reshape(batch_size,num_hyper,dimensions)\n",
    "        \n",
    "        return f_hat\n",
    "\n",
    "    def loss(\n",
    "            self,\n",
    "            f_hat:torch.tensor = None,\n",
    "            g_hat:torch.tensor = None,\n",
    "            f_var_hat:torch.tensor = None,\n",
    "            g_var_hat:torch.tensor = None,\n",
    "            mask:torch.tensor = None,\n",
    "            drift_at_hypercube:torch.tensor = None,\n",
    "            diffusion_at_hypercube:torch.tensor = None,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        obs_values, obs_times, diffusion_at_hypercube, drift_at_hypercube, hypercube_locations, mask\n",
    "        Compute the loss of the FIMODE_mix model (in original space).\n",
    "\n",
    "        The loss consists of supervised losses\n",
    "            - negative log-likelihood of the vector field values at fine grid points\n",
    "            - negative log-likelihood of the initial condition\n",
    "        and an unsupervised loss\n",
    "            - one-step ahead prediction loss.\n",
    "        The total loss is a weighted sum of all losses. The weights are defined in the loss_configs. (loss_scale_drift, loss_scale_init_cond, loss_scale_unsuperv_loss)\n",
    "\n",
    "        Args:\n",
    "            f_hat (tuple): mean and log standard deviation of the vector field concepts (in original space) ([B, L, D], [B, L, D])\n",
    "\n",
    "        Returns:\n",
    "            dict: llh_drift, llh_init_cond, unsupervised_loss, loss = weighted sum of all losses\n",
    "        \"\"\"\n",
    "        llh_drift = self.criterion(f_hat,drift_at_hypercube)\n",
    "        return llh_drift\n",
    "    \n",
    "    def training_step(\n",
    "            self, \n",
    "            batch, \n",
    "            batch_idx\n",
    "        ):\n",
    "        obs_values, obs_times, diffusion_at_hypercube, drift_at_hypercube, hypercube_locations, mask = batch\n",
    "        f_hats = self.forward(hypercube_locations=hypercube_locations,\n",
    "                               obs_values=obs_values,\n",
    "                               obs_times=obs_times,\n",
    "                               observation_mask=mask,\n",
    "                               training=True)\n",
    "        loss = self.loss(f_hats, \n",
    "                         drift_at_hypercube=drift_at_hypercube)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Hypercube Size: 1024\n",
      "Max Dimension: 3\n",
      "Max Num Steps: 129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cesar\\desktop\\projects\\foundationmodels\\fimodemix\\src\\fimodemix\\data\\datasets.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data: FIMSDEDatabatch = torch.load(file_path)  # Adjust loading method as necessary\n"
     ]
    }
   ],
   "source": [
    "# experiment\n",
    "experiment_files = ExperimentsFiles(experiment_indentifier=\"test\",delete=True)\n",
    "\n",
    "# Create dataset and DataLoader using ModelParams\n",
    "params = FIMSDEModelParams(seq_length=10, \n",
    "                           batch_size=32,\n",
    "                           num_epochs=2)\n",
    "\n",
    "# Define Data Set\n",
    "dataset = FIMSDEDataset(params=params)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=24, shuffle=True)\n",
    "databatch = next(data_loader.__iter__())\n",
    "obs_values, obs_times, diffusion_at_hypercube, drift_at_hypercube, hypercube_locations, mask = databatch\n",
    "\n",
    "batch_size = obs_times.size(0)\n",
    "num_steps = obs_times.size(1)\n",
    "dimensions = obs_values.size(2)\n",
    "num_hyper = hypercube_locations.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up TensorBoard logger\n",
    "logger = TensorBoardLogger(experiment_files.tensorboard_dir, \n",
    "                           name=\"time_series_transformer\")\n",
    "\n",
    "# Set up Model Checkpointing\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='train_loss',\n",
    "    dirpath=experiment_files.checkpoints_dir,\n",
    "    filename='best-checkpoint',\n",
    "    save_top_k=1,\n",
    "    mode='min',\n",
    "    save_weights_only=True,\n",
    "    every_n_train_steps=100  # Save checkpoint every 100 training steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cesar\\anaconda3\\envs\\fimode\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 1024, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model and train\n",
    "model = FIMSDE_p(\n",
    "    params\n",
    ")\n",
    "\n",
    "f_hats = model(hypercube_locations, \n",
    "            obs_values, \n",
    "            obs_times,\n",
    "            mask)\n",
    "f_hats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(151414.3281, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loss(f_hats, drift_at_hypercube=drift_at_hypercube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | phi_t0    | TimeEncoding       | 38     | train\n",
      "1 | phi_x0    | Mlp                | 3.8 K  | train\n",
      "2 | phi_1     | Mlp                | 2.9 K  | train\n",
      "3 | phi_2     | Mlp                | 4.8 K  | train\n",
      "4 | psi1      | TransformerModel   | 93.2 K | train\n",
      "5 | query_1x  | QueryGenerator     | 160    | train\n",
      "6 | query_1   | StaticQuery        | 5.2 K  | train\n",
      "7 | omega_1   | MultiheadAttention | 6.6 K  | train\n",
      "8 | criterion | MSELoss            | 0      | train\n",
      "---------------------------------------------------------\n",
      "116 K     Trainable params\n",
      "0         Non-trainable params\n",
      "116 K     Total params\n",
      "0.466     Total estimated model params size (MB)\n",
      "61        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\cesar\\anaconda3\\envs\\fimode\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|▏         | 8/417 [00:02<02:11,  3.12it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cesar\\anaconda3\\envs\\fimode\\Lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 417/417 [00:07<00:00, 52.63it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 417/417 [00:07<00:00, 52.63it/s, v_num=0]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    max_epochs=params.num_epochs,\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "trainer.fit(model, data_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fimode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
