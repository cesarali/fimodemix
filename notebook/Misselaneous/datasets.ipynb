{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class FIMSDEDatabatch:\n",
    "    obs_values: torch.Tensor\n",
    "    obs_times: torch.Tensor\n",
    "\n",
    "    diffusion_at_hypercube: torch.Tensor\n",
    "    drift_at_hypercube: torch.Tensor\n",
    "    hypercube_locations: torch.Tensor\n",
    "\n",
    "    diffusion_parameters: torch.Tensor\n",
    "    drift_parameters: torch.Tensor\n",
    "    init_condition_distr_parameters: torch.Tensor\n",
    "\n",
    "    f_strs: torch.Tensor = None\n",
    "    g_strs: torch.Tensor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['damped.tr', 'lorenz.tr']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"../../data/parameters_sde/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cesar\\AppData\\Local\\Temp\\ipykernel_21184\\1852337665.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data_lorenz = torch.load(\"../../data/parameters_sde/lorenz.tr\")\n",
      "C:\\Users\\cesar\\AppData\\Local\\Temp\\ipykernel_21184\\1852337665.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data_damped = torch.load(\"../../data/parameters_sde/damped.tr\")\n"
     ]
    }
   ],
   "source": [
    "data_lorenz = torch.load(\"../../data/parameters_sde/lorenz.tr\")\n",
    "data_damped = torch.load(\"../../data/parameters_sde/damped.tr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000.*0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "from fimodemix import data_path\n",
    "from fimodemix.data.generation_sde import generate_data\n",
    "\n",
    "class FIMSDEDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This Dataset performs on-the-fly dimension padding.\n",
    "    \"\"\"\n",
    "    def __init__(self, file_paths=None):\n",
    "        if file_paths is None:\n",
    "            data_path = Path(damped_path)\n",
    "            lorenz_path = data_path / \"parameters_sde\" / \"lorenz.tr\"\n",
    "            damped_path = data_path / \"parameters_sde\" / \"damped.tr\"\n",
    "            file_paths = [lorenz_path,damped_path]\n",
    "            if not lorenz_path.exists():\n",
    "                generate_data()\n",
    "\n",
    "        # To keep track of the number of samples in each file\n",
    "        self.data = []\n",
    "        self.lengths = []  \n",
    "        # Load data and compute cumulative lengths\n",
    "        self.read_files(file_paths)\n",
    "        self.cumulative_lengths = np.cumsum(self.lengths)\n",
    "        \n",
    "    def read_files(self, file_paths):\n",
    "        self.max_dimension = 1\n",
    "        self.max_hypercube_size = 100\n",
    "        for file_path in file_paths:\n",
    "            data: FIMSDEDatabatch = torch.load(file_path)  # Adjust loading method as necessary\n",
    "            self.data.append(data)\n",
    "            self.lengths.append(data.obs_values.size(0))  # Number of samples in this file\n",
    "            # Update max dimensions\n",
    "            self.max_dimension = max(self.max_dimension, data.obs_values.size(2))\n",
    "            self.max_hypercube_size = max(self.max_hypercube_size, data.diffusion_at_hypercube.size(1))\n",
    "        print(f'Max Hypercube Size: {self.max_hypercube_size}')\n",
    "        print(f'Max Dimension: {self.max_dimension}')\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(self.lengths)  # Total number of samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, sample_idx = self._get_file_and_sample_index(idx)\n",
    "\n",
    "        # Get the tensor from the appropriate file\n",
    "        obs_values = self.data[file_idx].obs_values[sample_idx]\n",
    "        obs_times = self.data[file_idx].obs_times[sample_idx]\n",
    "        diffusion_at_hypercube = self.data[file_idx].diffusion_at_hypercube[sample_idx]\n",
    "        drift_at_hypercube = self.data[file_idx].drift_at_hypercube[sample_idx]\n",
    "        hypercube_locations = self.data[file_idx].hypercube_locations[sample_idx]\n",
    "\n",
    "        # Pad the tensors if necessary\n",
    "        obs_values, diffusion_at_hypercube, drift_at_hypercube, hypercube_locations, mask = self._pad_tensors(\n",
    "            obs_values, diffusion_at_hypercube, drift_at_hypercube, hypercube_locations\n",
    "        )\n",
    "\n",
    "        return obs_values, obs_times, diffusion_at_hypercube, drift_at_hypercube, hypercube_locations, mask\n",
    "\n",
    "    def _get_file_and_sample_index(self, idx):\n",
    "        \"\"\"Helper function to determine the file index and sample index.\"\"\"\n",
    "        file_idx = np.searchsorted(self.cumulative_lengths, idx)\n",
    "        sample_idx = idx if file_idx == 0 else idx - self.cumulative_lengths[file_idx - 1]\n",
    "        return file_idx, sample_idx\n",
    "\n",
    "    def _pad_tensors(self, obs_values, diffusion_at_hypercube, drift_at_hypercube, hypercube_locations):\n",
    "        \"\"\"Pad the tensors to ensure they meet the expected dimensions.\"\"\"\n",
    "        current_dimension = obs_values.size(1)\n",
    "        current_hyper = drift_at_hypercube.size(0)\n",
    "\n",
    "        dim_padding_size = self.max_dimension - current_dimension\n",
    "        hyper_padding_size = self.max_hypercube_size - current_hyper\n",
    "\n",
    "        if dim_padding_size > 0 or hyper_padding_size > 0:\n",
    "            obs_values = torch.nn.functional.pad(obs_values, (0, dim_padding_size))\n",
    "            diffusion_at_hypercube = torch.nn.functional.pad(diffusion_at_hypercube, (0, dim_padding_size, 0, hyper_padding_size))\n",
    "            drift_at_hypercube = torch.nn.functional.pad(drift_at_hypercube, (0, dim_padding_size, 0, hyper_padding_size))\n",
    "            hypercube_locations = torch.nn.functional.pad(hypercube_locations, (0, dim_padding_size, 0, hyper_padding_size))\n",
    "            mask = self._create_mask(obs_values, current_dimension)\n",
    "        else:\n",
    "            mask = torch.ones_like(obs_values)\n",
    "            \n",
    "        return obs_values, diffusion_at_hypercube, drift_at_hypercube, hypercube_locations, mask\n",
    "\n",
    "    def _create_mask(self, obs_values, current_dimension):\n",
    "        \"\"\"Create a mask for the observations.\"\"\"\n",
    "        mask = torch.ones_like(obs_values)\n",
    "        mask[:, current_dimension:] = 0.\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Hypercube Size: 1024\n",
      "Max Dimension: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cesar\\AppData\\Local\\Temp\\ipykernel_21184\\3322852728.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data: FIMSDEDatabatch = torch.load(file_path)  # Adjust loading method as necessary\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "dataset = FIMSDEDataset()\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=24, shuffle=True)\n",
    "databatch = next(data_loader.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fimode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
