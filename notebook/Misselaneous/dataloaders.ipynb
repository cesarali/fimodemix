{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5(path: Path):\n",
    "    \"Array in .h5 is under 'data' key\"\n",
    "    arr = None\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        data = f[\"data\"][:]\n",
    "        try:\n",
    "            # Attempt to convert the data to floats\n",
    "            arr = np.array(data, dtype=\"float32\").T  # Transpose for C-order\n",
    "        except ValueError:\n",
    "            # If the conversion fails, keep the data as a string\n",
    "            arr = np.array(data, dtype=str)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['drift_functions_at_hypercube.h5',\n",
       " 'f_strs.h5',\n",
       " 'g_strs.h5',\n",
       " 'hypercube_locations.h5',\n",
       " 'init_condition_distr_parameters.h5',\n",
       " 'obs_times.h5',\n",
       " 'obs_values.h5',\n",
       " 'scaled_diffusion_functions_at_hypercube.h5']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = r\"C:\\Users\\cesar\\Desktop\\Projects\\FoundationModels\\Data\\data-snr_01_05_1_5\\data-snr_01_05_1_5\\linear\\dim-1\\2\"\n",
    "batchdata_dir_path = Path(data_dir)\n",
    "file_names = os.listdir(data_dir)\n",
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all files\n",
    "loaded_data = {}\n",
    "for file_name in file_names:\n",
    "    file_path = batchdata_dir_path / file_name\n",
    "    file_name_ = file_path.name.removesuffix(\".h5\")\n",
    "    loaded_data[file_name_] = load_h5(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['drift_functions_at_hypercube', 'f_strs', 'g_strs', 'hypercube_locations', 'init_condition_distr_parameters', 'obs_times', 'obs_values', 'scaled_diffusion_functions_at_hypercube'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_times = loaded_data['obs_times']\n",
    "obs_values =  loaded_data['obs_values']\n",
    "f_strs = loaded_data['f_strs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4474, 300, 128, 1, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chop data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 100 entries from each file have been saved.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the directories\n",
    "data_dir = r'C:\\Users\\cesar\\Desktop\\Projects\\FoundationModels\\Data\\data-snr_01_05_1_5\\data-snr_01_05_1_5\\linear\\dim-1\\1'  # Your source directory\n",
    "output_dir = r'C:\\Users\\cesar\\Desktop\\Projects\\FoundationModels\\Data\\data-snr_01_05_1_5\\data-snr_01_05_1_5\\linear\\dim-1\\taquito2'  # Your destination directory\n",
    "\n",
    "# List of .h5 files\n",
    "h5_files = [\n",
    "    'drift_functions_at_hypercube.h5',\n",
    "    'f_strs.h5',\n",
    "    'g_strs.h5',\n",
    "    'hypercube_locations.h5',\n",
    "    'init_condition_distr_parameters.h5',\n",
    "    'obs_times.h5',\n",
    "    'obs_values.h5',\n",
    "    'scaled_diffusion_functions_at_hypercube.h5'\n",
    "]\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each file\n",
    "for file_name in h5_files:\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    output_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "    # Open the source file\n",
    "    with h5py.File(file_path, 'r') as h5_file:\n",
    "        # Create a new HDF5 file to save the first 100 entries\n",
    "        with h5py.File(output_path, 'w') as output_file:\n",
    "            # Iterate over each dataset in the file\n",
    "            for dataset_name in h5_file.keys():\n",
    "                dataset = h5_file[dataset_name][:]\n",
    "                \n",
    "                # Select the first 100 entries\n",
    "                if len(dataset) > 100:\n",
    "                    dataset = dataset[:100]\n",
    "                \n",
    "                # Save the modified dataset\n",
    "                output_file.create_dataset(dataset_name, data=dataset)\n",
    "\n",
    "print(\"First 100 entries from each file have been saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fimode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
